<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Mengchu Li" />
  <meta name="author" content="Lukas Trottner" />
  <title>Lecture notes Foundations of Statistical Inference</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title"><span class="nodecor">Lecture notes</span><br />
<strong>Foundations of<br />
Statistical Inference</strong></h1>
<p class="author">Mengchu Li</p>
<p class="author">Lukas Trottner</p>
<p class="date">Autumn Term 2024<br />
2024-11-08<br />
- <em>Preliminary version</em> -<br />
<img src="UoB_Logo.png" alt="image" /></p>
</header>
<p>1</p>
<p>1</p>
<h1 id="fundamental-concepts-in-inference">Fundamental concepts in
inference</h1>
<h2 id="introduction">Introduction</h2>
<p>Suppose that a set of <span class="math inline">\(n\)</span> data
points <span class="math inline">\(\bm{x} = \{x_1,\dotsc,x_n\}\)</span>
are collected in an experiment to estimate some unknown quantity.
Statisticians view <span class="math inline">\(\bm{x}\)</span> as
realisations from random variables <span class="math inline">\(\bm{X} =
\{X_1,\dotsc, X_n\}\)</span>. Throughout the rest of this course, we
will focus on the case where <span class="math inline">\(X_1,\dotsc,
X_n\)</span> are i.i.d with some distribution <span
class="math inline">\(P\)</span>. The goal of statistical inference is
to learn about the data generating mechanism <span
class="math inline">\(P\)</span>, using the available sample <span
class="math inline">\(\bm{x}\)</span>.</p>
<div class="example">
<p><em>Example 1.1</em>.  </p>
<ul>
<li><p>Coin flipping;</p></li>
<li><p>Quality control in manufacturing;</p></li>
<li><p>Clinical trials for new drug efficacy;</p></li>
<li><p>Language ability tests in a school;</p></li>
<li><p>Tracking someone’s weight over time; (the i.i.d assumption is
unlikely to be true in this case; <em>Time series analysis</em> is
predominately concerned with statistical inference under temporal
dependence, which relax the independence assumption)</p></li>
<li><p>...</p></li>
</ul>
</div>
<div class="remark">
<p><em>Remark 1.2</em>. To distinguish quantities that are deterministic
and random, we mostly use lowercase letters to denote deterministic
quantities, e.g. <span class="math inline">\({x}\)</span>, and uppercase
letters to denote random variables, e.g. <span
class="math inline">\(X\)</span>. In particular, for deterministic
quantities, we have <span class="math inline">\(\mathbb{E}[x] =
x\)</span> and Var<span class="math inline">\((x) = 0\)</span>.</p>
</div>
<p>We will work under the assumption that <span
class="math inline">\(P\)</span> belongs to some statistical model that
can be parameterised by some parameter <span
class="math inline">\(\theta\)</span>, as defined below. Again, there
are many situations where such an assumption may not be appropriate -
see the course on <em>Nonparametric Statistics</em> for further
discussions.</p>
<div class="definition">
<p><strong>Definition 1.3</strong> (Statistical model). Let <span
class="math inline">\(\Theta\)</span> denote the parameter space and
<span class="math inline">\(\mathcal{S}\)</span> denote the sample
space. A statistical model is a family of probability distributions on
<span class="math inline">\(\mathcal{S}\)</span>. A parametric
statistical model is a set of distributions parametrised by <span
class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}^d\)</span>,
written as <span class="math inline">\(\{P_\theta: \theta \in
\Theta\}\)</span>.</p>
</div>
<div class="example">
<p><em>Example 1.4</em>. Some examples of statistical models</p>
<ul>
<li><p><span class="math inline">\(\{N(\theta,1) : \theta \in \Theta =
\mathbb{R}\}\)</span>;</p></li>
<li><p><span class="math inline">\(\{N(\theta,1) : \theta \in \Theta =
[-2,5]\}\)</span>;</p></li>
<li><p><span class="math inline">\(\{N(\mu, \sigma^2I_d) : \theta =
(\mu,\sigma) \in \Theta = \mathbb{R}^{d+1}\}\)</span>;</p></li>
<li><p>{Ber<span class="math inline">\((\theta): \theta \in
[0,1]\)</span>}.</p></li>
</ul>
</div>
<p>We shall write <span class="math display">\[\label{eq:i.i.d}
    X_1,\dotsc,X_n \overset{i.i.d}{\sim} P_{\theta}, \; \theta \in
\Theta,\]</span> to denote that random variables <span
class="math inline">\(X_1,\ldots,X_n \colon \Omega
\to\mathcal{S}\)</span> are independent and identically distributed
(i.i.d.) with distribution <span
class="math inline">\(P_{\theta}\)</span>. Recall that this means for
some probability measure <span
class="math inline">\(\mathbb{P}_\theta\)</span> on <span
class="math inline">\((\Omega,\mathcal{F})\)</span>, <span
class="math inline">\(X_1,\ldots,X_n\)</span> are i.i.d. with
distribution <span
class="math inline">\(\mathbb{P}_\theta(X^{-1}(\cdot)) =
P_\theta(\cdot)\)</span>. We shall always assume that <span
class="math inline">\(P_\theta\)</span> is dominated by some (<span
class="math inline">\(\sigma\)</span>-finite) measure <span
class="math inline">\(\mu\)</span> on <span
class="math inline">\(\mathcal{S}\)</span>, and therefore it is
equivalent to write <a href="#eq:i.i.d" data-reference-type="eqref"
data-reference="eq:i.i.d">[eq:i.i.d]</a> as <span
class="math display">\[X_1,\dotsc,X_n \overset{i.i.d}{\sim} f(x;
{\theta}),\; \theta \in \Theta,\]</span> where <span
class="math inline">\(f(x; {\theta}) =
\frac{dP_\theta}{d\mu}(x)\)</span>. In statistics, the measure <span
class="math inline">\(\mu\)</span> is often the Lebesgue measure (for
continuous random variables) or the counting measure (for discrete
random variables).</p>
<div class="definition">
<p><strong>Definition 1.5</strong> (Statistic). Given <span
class="math inline">\({X} = \{X_1,\ldots,X_n\}\)</span>, any
(measurable) function of <span class="math inline">\({X}\)</span> that
does not depend on unknown quantities is called a
<em>statistic</em>.</p>
</div>
<div class="remark">
<p><em>Remark 1.6</em>. We often use <span
class="math inline">\(\hat{\theta}_n({X})\)</span> to denote a
statistic. Statistics are computed mostly to estimate the unknown
quantity <span class="math inline">\(\theta\)</span> in the data
generating mechanism <span class="math inline">\(P_\theta\)</span>, and
therefore we often also call it an <em>estimator</em>. Note that the
subscript <span class="math inline">\(n\)</span> in <span
class="math inline">\(\hat{\theta}_n(X)\)</span> indicate that it is a
function of the sample size <span class="math inline">\(n\)</span>. Note
also that it is a random variable since it depends on <span
class="math inline">\(X\)</span> which is random, whereas <span
class="math inline">\(\theta\)</span> is a deterministic quantity in
this course. We will explore the possibility of viewing <span
class="math inline">\(\theta\)</span> as a random variable in the
<em>Bayesian Inference and Computation</em> course.</p>
</div>
<div class="example">
<p><em>Example 1.7</em>. Some examples of statistics</p>
<ul>
<li><p><span class="math inline">\(\hat{\theta}_n({X}) =
{X}\)</span>;</p></li>
<li><p><span class="math inline">\(\hat{\theta}_n(X) =
\frac{1}{n}\sum_{i=1}^nX_i\)</span></p></li>
<li><p><span class="math inline">\(\hat{\theta}_n(X) =\)</span>
median<span class="math inline">\(({X})\)</span></p></li>
</ul>
</div>
<p>Most statistical problems can be identified as one of following three
types:</p>
<ul>
<li><p>(Point) Estimation: Constructing <span
class="math inline">\(\hat{\theta}_n(X)\)</span> such that for all <span
class="math inline">\(\theta \in \Theta\)</span>, when <span
class="math inline">\(X_i \sim P_\theta\)</span>, our estimator <span
class="math inline">\(\hat{\theta}_n\)</span> is close to <span
class="math inline">\(\theta\)</span>.</p></li>
<li><p>(Hypothesis) Testing: Making decisions about whether we are under
the null hypothesis <span class="math inline">\(H_0\)</span> or the
alternative hypothesis <span class="math inline">\(H_1\)</span> using
some test statistic <span class="math inline">\(T_n(X)\)</span> that
takes value <span class="math inline">\(0\)</span> or <span
class="math inline">\(1\)</span>. The goal is constructing <span
class="math inline">\(T_n(X)\)</span> such that <span
class="math inline">\(T_n(X) = 0\)</span> when <span
class="math inline">\(H_0\)</span> is true and <span
class="math inline">\(T_n(X) = 1\)</span> when <span
class="math inline">\(H_1\)</span> is true, with high
probability.</p></li>
<li><p>Confidence sets: Finding sets or intervals (depending on the
dimensionality of the parameter) <span
class="math inline">\(C_n(X)\)</span> such that for <span
class="math inline">\(\alpha \in (0,1)\)</span> we have <span
class="math inline">\(\mathbb{P}_{\theta}(\theta \in C_n(X)) \geq
1-\alpha\)</span>, for all <span class="math inline">\(\theta \in
\Theta\)</span>.</p></li>
</ul>
<h2 id="point-estimation">Point estimation</h2>
<p>We consider the task of estimating the parameter <span
class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}\)</span>.
Given an an i.i.d. sample <span
class="math inline">\(X_1,\ldots,X_n\)</span> for a statistical model
<span class="math inline">\((P_\theta)_{\theta \in \Theta}\)</span>, an
<em>estimator</em> <span
class="math inline">\(\hat{\theta}_n(X_1,\ldots,X_n)\)</span> is a
statistic that is designed to estimate the unknown parameter of interest
<span class="math inline">\(\theta\)</span>. To assess the quantity of
the estimator <span class="math inline">\(\hat{\theta}_n\)</span>, we
may consider the following quantities.</p>
<div class="definition">
<p><strong>Definition 1.8</strong> (Bias and mean squared error).  </p>
<ol>
<li><p>We call <span
class="math display">\[\operatorname{Bias}(\hat{\theta}_n)
\coloneq\mathbb{E}[\hat{\theta}_n] - \theta\]</span> the <em>bias</em>
of <span class="math inline">\(\hat{\theta}_n\)</span> (w.r.t. the
unknown parameter <span class="math inline">\(\theta\)</span>). We say
that <span class="math inline">\(\hat{\theta}_n\)</span> is
<em>unbiased</em>, if <span class="math display">\[\forall \theta \in
\Theta: \operatorname{Bias}(\hat{\theta}_n) = 0.\]</span></p>
<p>Note that to compute the bias, we need to compute <span
class="math inline">\(\mathbb{E}[\hat{\theta}_n] = \int_{\mathbb{R}^n}
\hat{\theta}_n(x_1,\ldots,x_n) d P^n_\theta(x_1,\ldots,x_n)\)</span>
which usually leads to a function of the unknown parameter <span
class="math inline">\(\theta\)</span>. Sometimes people use <span
class="math inline">\(\mathbb{E}_{\theta}\)</span> to emphasize the fact
that the random variables have distribution <span
class="math inline">\(P_\theta\)</span>, but we simply write <span
class="math inline">\(\mathbb{E}\)</span> as long as the source of
randomness is clear. Moreover, using properties of expectation
(especially linearity) can often simplify the calculation.</p></li>
<li><p>We call <span
class="math display">\[\operatorname{MSE}(\hat{\theta}_n)
\coloneq\mathbb{E}( \hat{\theta}_n - \theta)^2\]</span> the <em>mean
squared error</em> of <span
class="math inline">\(\hat{\theta}_n\)</span> (w.r.t. the unknown
parameter <span class="math inline">\(\theta\)</span>). As we will show
shortly, computing MSE is equivalent to computing the bias and variance
of <span class="math inline">\(\hat{\theta}_n\)</span>.</p></li>
</ol>
</div>
<div class="example">
<p><em>Example 1.9</em>. Let <span class="math inline">\(P_\theta =
\mathcal{N}(\theta,\sigma^2)\)</span> for <span
class="math inline">\(\theta \in \mathbb{R}\)</span> and variance <span
class="math inline">\(\sigma^2\)</span>. Let <span
class="math inline">\(X_1,X_2,\ldots,X_n\)</span> be an i.i.d. sample
from <span class="math inline">\(P_\theta\)</span> and define <span
class="math inline">\(\hat{\theta}_n \coloneq\overline{X}_n =
\sum_{i=1}^nX_i/n\)</span>. For any <span class="math inline">\(\theta
\in \mathbb{R}\)</span>, <span
class="math inline">\(\mathbb{E}[\hat{\theta}_n] =
\mathbb{E}[\overline{X}_n] = \theta\)</span>, so the sample mean <span
class="math inline">\(\overline{X}_n\)</span> is an unbiased estimator
for <span class="math inline">\(\theta\)</span>. For the mean squared
error, note that <span class="math display">\[\mathbb{E}( \hat{\theta}_n
- \theta)^2  = \mathbb{E}(\overline{X}_n-\theta)^2 =
\mathbb{E}(\overline{X}_n-\mathbb{E}(\overline{X}_n))^2 =
\text{Var}(\overline{X}_n) = \sigma^2/n.\]</span></p>
</div>
<div id="lemma:bias-variance" class="lemma">
<p><strong>Lemma 1.10</strong> (Bias-variance decomposition). <em>Let
<span class="math inline">\(\hat{\theta}_n\)</span> be an estimator for
the unknown parameter <span class="math inline">\(\theta\)</span> of a
statistical model <span class="math inline">\((P_\theta)_{\theta \in
\Theta}\)</span>. It holds that <span class="math display">\[\forall
\theta \in \Theta: \quad \operatorname{MSE}(\hat{\theta}_n) =
(\operatorname{Bias}(\hat{\theta}_n))^2 +
\operatorname{Var}(\hat{\theta}_n).\]</span></em></p>
</div>
<div class="proof">
<p><em>Proof.</em> <span class="math display">\[\begin{aligned}
\operatorname{MSE}(\hat{\theta}_n) &amp;= \mathbb{E}\big[\lvert
(\hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]) +
(\mathbb{E}[\hat{\theta}_n] - \theta)\rvert^2\big] \\
&amp;=\mathbb{E}\big[\lvert \hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]
\rvert^2 \big]
+ 2\underbrace{\mathbb{E}\big[(\hat{\theta}_n -
\mathbb{E}[\hat{\theta}_n])(\mathbb{E}[\hat{\theta}_n] - \theta)
\big]}_{\mathclap{=\underbrace{\mathbb{E}[(\hat{\theta}_n -
\mathbb{E}[\hat{\theta}_n])]}_{\mathclap{= \mathbb{E}[\hat{\theta}_n] -
\mathbb{E}[\hat{\theta}_n] = 0}}\textstyle{(\mathbb{E}[\hat{\theta}_n] -
\theta)}}} +\underbrace{\mathbb{E}\big[\lvert \mathbb{E}[\hat{\theta}_n]
- \theta \rvert^2 \big]}_{\textstyle= \lvert \mathbb{E}[\hat{\theta}_n]
- \theta \rvert^2} \\
&amp;=\mathbb{E}\big[\lvert \hat{\theta}_n - \mathbb{E}[\hat{\theta}_n]
\rvert^2 \big]  +  \lvert \mathbb{E}[\hat{\theta}_n] - \theta \rvert^2\\
&amp;= \mathop{\mathrm{Var}}(\hat{\theta}_n) +
\big(\operatorname{Bias}(\hat{\theta}_n)\big)^2
\end{aligned}\]</span> ◻</p>
</div>
<div id="example:expo1" class="example">
<p><em>Example 1.11</em>. Let <span class="math inline">\(P_\theta =
\operatorname{Exp}(\theta)\)</span> for <span
class="math inline">\(\theta \in (0,\infty)\)</span>, i.e. <span
class="math display">\[f(x;\theta) = \theta e^{-\theta x}, x \geq 0,
\theta &gt;0.\]</span> Suppose <span class="math inline">\(n &gt;
2\)</span>. Consider <span class="math inline">\(\hat{\theta}_n =
\tfrac{1}{\overline{X}_n}\)</span> and we would like to compute its bias
and mean squared error. Since <span
class="math inline">\(X_1,\dotsc,X_n\)</span> are i.i.d Exp(<span
class="math inline">\(\theta\)</span>), we have <span
class="math inline">\(\sum_{i=1}^n X_i\)</span> has distribution <span
class="math inline">\(\Gamma(n,\theta)\)</span> (See Proposition 3.2.18)
with density function <span class="math inline">\(f_{n,\theta}\)</span>.
Therefore, <span class="math display">\[\begin{aligned}
         \mathbb{E}\mathopen{}\mathclose\bgroup\left[\frac{1}{\overline{X}_n}\aftergroup\egroup\right]
&amp;= \mathbb{E}\mathopen{}\mathclose\bgroup\left[\frac{n}{\sum_{i=1}^n
X_i}\aftergroup\egroup\right] = \int_0^{\infty} \frac{n}{y}
f_{n,\theta}(y) dy = \int_0^{\infty}
\frac{n}{y}\frac{\theta^n}{\Gamma(n)}y^{n-1}e^{-\theta y} dy \\
         &amp;= \frac{n\theta\Gamma(n-1)}{\Gamma(n)}
\int_0^{\infty}\frac{\theta^{n-1}}{\Gamma(n-1)}y^{n-2}e^{-\theta y} dy =
\frac{n}{n-1} \theta \int_0^{\infty} f_{n-1,\theta}(y) dy =
\frac{n}{n-1} \theta.
    
\end{aligned}\]</span> Then we have <span
class="math display">\[\text{Bias}(\hat{\theta}_n) = \frac{n}{n-1}\theta
- \theta = \frac{1}{n-1}\theta.\]</span> We leave it as an exercise to
show that <span class="math display">\[\text{Var}(\hat{\theta}_n) =
\frac{n^2\theta^2}{(n-1)^2(n-2)}.\]</span> Combining the results for
bias and variance of <span
class="math inline">\(\hat{\theta}_n\)</span>, we can conclude using
Lemma <a href="#lemma:bias-variance" data-reference-type="ref"
data-reference="lemma:bias-variance">1.10</a> that <span
class="math display">\[\text{MSE}(\hat{\theta}_n) =
\text{Bias}(\hat{\theta}_n)^2+\text{Var}(\hat{\theta}_n) =
\mathopen{}\mathclose\bgroup\left(\frac{1}{n-1}\theta\aftergroup\egroup\right)^2
+ \frac{n^2\theta^2}{(n-1)^2(n-2)} =
\frac{n+2}{(n-1)(n-2)}\theta^2.\]</span></p>
</div>
<p>Bias, variance, and mean squared error are all important quantities
regarding an estimator that we might be interested in. In particular,
they describe different aspects of the finite-sample (non-asymptotic)
behavior of an estimator. We are also interested in asymptotic
properties of an estimator, which describes the behavior of an estimator
as the sample size <span class="math inline">\(n\)</span> becomes
infinite. The power of asymptotic evaluation is that when we let the
sample size becomes infinite, calculations simplify.</p>
<div class="definition">
<p><strong>Definition 1.12</strong> (Consistency and Asymptotics).  </p>
<ol>
<li><p>We say that <span class="math inline">\(\hat{\theta}_n\)</span>
is <em>consistent</em>, if <span class="math display">\[\forall \theta
\in \Theta: \hat{\theta}_n \overset{\mathbb{P}_\theta}{\longrightarrow}
\theta \quad \Big(\iff \forall \theta \in \Theta,\varepsilon &gt; 0:
\mathbb{P}_\theta(\lvert \hat{\theta}_n - \theta \rvert \geq
\varepsilon) \underset{n \to \infty}{\longrightarrow} 0
\Big)\]</span></p></li>
<li><p>We say <span class="math inline">\(\hat{\theta}_n\)</span> is
<em>mean square consistent</em>, if <span
class="math display">\[\text{MSE}(\hat{\theta}_n) \underset{n \to
\infty}{\longrightarrow} 0,\]</span> and <span
class="math inline">\(\hat{\theta}_n\)</span> is <em>asymptotically
unbiased</em> if <span
class="math display">\[\text{Bias}(\hat{\theta}_n) \underset{n \to
\infty}{\longrightarrow} 0.\]</span></p></li>
</ol>
</div>
<div id="prop:mse-consistent" class="proposition">
<p><strong>Proposition 1.13</strong>. <em>Let <span
class="math inline">\((\hat{\theta}_n)_{n \in \mathbb{N}}\)</span> be a
sequence of estimators for the unknown parameter <span
class="math inline">\(\theta\)</span> of a statistical model <span
class="math inline">\((P_\theta)_{\theta \in \Theta}\)</span>. If <span
class="math inline">\(\operatorname{MSE}(\hat{\theta}_n) \underset{n \to
\infty}{\longrightarrow} 0\)</span> for all <span
class="math inline">\(\theta \in \Theta\)</span>, then <span
class="math inline">\((\hat{\theta}_n)_{n \in \mathbb{N}}\)</span> is
consistent.</em></p>
</div>
<div class="proof">
<p><em>Proof.</em> It is a direct consequence of convergence in <span
class="math inline">\(\mathcal{L}^2\)</span> implies convergence in
probability (Theorem 4.1.2). We repeat this proof here again. For all
<span class="math inline">\(\theta \in \Theta\)</span> and <span
class="math inline">\(\varepsilon &gt; 0\)</span>, Markov’s inequality
gives <span class="math display">\[\mathbb{P}_\theta(\lvert
\hat{\theta}_n - \theta \rvert \geq \varepsilon) \leq
\frac{\mathbb{E}[\lvert \hat{\theta}_n - \theta
\rvert^2]}{\varepsilon^2} =
\frac{\operatorname{MSE}(\hat{\theta}_n)}{\varepsilon^2} \underset{n \to
\infty}{\longrightarrow} 0.\]</span> This establishes <span
class="math inline">\(\hat{\theta}_n
\overset{\mathbb{P}}{\longrightarrow} \theta\)</span> for all <span
class="math inline">\(\theta \in \Theta\)</span>, i.e.,
consistency. ◻</p>
</div>
<div id="ex:var_mse" class="example">
<p><em>Example 1.14</em>. Let <span class="math inline">\(P_{\theta} =
\mathcal{N}(0,\theta^2)\)</span> for <span
class="math inline">\(\theta^2 \in (0,\infty)\)</span> and let <span
class="math inline">\(X_1,X_2,\ldots, X_n \overset{i.i.d}{\sim}
P_\theta\)</span>. Consider <span class="math inline">\(\hat{\theta}^2_n
\coloneq\frac{1}{n}\sum_{i=1}^n X_i^2\)</span>, we have immediately
<span class="math display">\[\mathbb{E}[\hat{\theta}^2_n] =
\mathbb{E}[X_1^2] = \theta^2,\]</span> and therefore it is an unbiased
estimator for <span class="math inline">\(\theta^2\)</span>. Moreover,
since <span class="math inline">\(\theta^2 &lt; \infty\)</span>, by the
strong LLN, for any <span class="math inline">\(\theta^2 \in
(0,\infty)\)</span> it holds that <span
class="math inline">\(\hat{\theta}^2_n
\overset{\text{a.s.}}{\longrightarrow} \theta^2\)</span> and therefore
also <span class="math inline">\(\hat{\theta}^2_n
\overset{\mathbb{P}}{\longrightarrow} \theta^2\)</span>, so <span
class="math inline">\(\hat{\theta}^2_n\)</span> is a consistent
estimator for <span class="math inline">\(\theta^2\)</span>.</p>
<p>Now, consider <span class="math inline">\(\hat{\theta}_n
\coloneq\sqrt{\tfrac{1}{n}\sum_{i=1}^n X_i^2}\)</span>. By the
continuous mapping theorem, it holds that <span
class="math display">\[\forall \theta \in \Theta: \quad \hat{\theta}_n
\overset{\mathbb{P}}{\longrightarrow} \theta.\]</span> If we tried to
show the consistency of <span
class="math inline">\(\hat{\theta}_n\)</span> for <span
class="math inline">\(\theta\)</span> by computing its MSE, it would be
much more difficult.</p>
</div>
<p>We can draw the following conclusion between various concepts
introduced so far. Suppose <span class="math inline">\(X_1,\dotsc, X_n
\overset{i.i.d}{\sim} P_\theta\)</span>. All the statements below are
regarding the parameter <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(\hat{\theta}_n\)</span> is
unbiased, then it is asymptotically unbiased. The converse is not true
(See Example <a href="#example:expo1" data-reference-type="ref"
data-reference="example:expo1">1.11</a>).</p></li>
<li><p>If <span class="math inline">\(\hat{\theta}_n\)</span> is mean
square consistent, then it is consistent (Proposition <a
href="#prop:mse-consistent" data-reference-type="ref"
data-reference="prop:mse-consistent">1.13</a>). The converse is not
true.</p></li>
<li><p>If <span class="math inline">\(\hat{\theta}_n\)</span> is mean
square consistent, then it is asymptotically unbiased.</p></li>
<li><p>Asymptotically unbiased and consistency generally do not imply
each other. (See exercises below).</p></li>
</ul>
<div class="exercise">
<p><em>Exercise 1.15</em>. Show that if <span
class="math inline">\(X_1,\dotsc, X_n \overset{i.i.d}{\sim}
\text{Exp}(\theta)\)</span>, then <span class="math inline">\(X_{\min} =
\min\{X_1,\dotsc,X_n\}\)</span> has distribution Exp(<span
class="math inline">\(n\theta\)</span>). Use this to show that <span
class="math inline">\(T_n(X) = nX_{\min}\)</span> is unbiased for <span
class="math inline">\(1/\theta\)</span>, i.e. <span
class="math inline">\(\mathbb{E}( nX_{\min}) = 1/\theta\)</span>. Also
compute <span class="math inline">\(\text{Var}(T_n(X))\)</span> and
conclude that it is not mean square consistent for <span
class="math inline">\(1/\theta\)</span>.</p>
</div>
<p>Given the result from the above exercise, we have for any <span
class="math inline">\(\varepsilon&gt; 0\)</span> <span
class="math display">\[\mathbb{P}_\theta(|nX_{\min} - 1/\theta| &gt;
\varepsilon) \geq \mathbb{P}_\theta(nX_{\min} - 1/\theta &gt;
\varepsilon) = \mathbb{P}_\theta(X_{\min} &gt; (\varepsilon+
(1/\theta))/n) = \exp(-(\theta \varepsilon+ 1),\]</span> and therefore
we can conclude that <span class="math inline">\(nX_{\min}\)</span> is
an unbiased estimator for <span class="math inline">\(1/\theta\)</span>,
but it is not consistent.</p>
<div class="exercise">
<p><em>Exercise 1.16</em> (*). Suppose <span
class="math inline">\(X_1,\dotsc,X_n \overset{i.i.d}{\sim} P_\theta,
\theta \in \Theta \subseteq \mathbb{R}\)</span>, and suppose we have an
estimator <span class="math inline">\(\hat{\theta}_n\)</span> that is
consistent and <span class="math inline">\(\mathbb{E}(\hat{\theta}^2_n)
\leq C\)</span>, for some constant <span
class="math inline">\(C&gt;0\)</span> and any <span
class="math inline">\(n \in \mathbb{N}\)</span>. Show that <span
class="math inline">\(\hat{\theta}_n\)</span> is also asymptotically
unbiased.</p>
</div>
<p>Having discussed various desirable properties of an estimator, we
will introduce a principled way of deriving estimators that enjoy some
of these properties (especially from an asymptotic point of view) in the
next chapter. It is known as the <em>Maximum Likelihood Estimator</em>
(MLE).</p>
<h2 id="confidence-intervals">Confidence intervals</h2>
<p>A confidence interval is a random interval <span
class="math inline">\(C\)</span> expressed in terms of the data <span
class="math inline">\(X_1,\ldots,X_n\)</span> such that with a certain
degree of confidence – that needs to be made precise – contains the
unknown parameter <span class="math inline">\(\theta\)</span>.</p>
<div class="definition">
<p><strong>Definition 1.17</strong>. Let <span
class="math inline">\((P_\theta)_{\theta \in \Theta}\)</span> be a
statistical model with parameter space <span
class="math inline">\(\Theta \subset \mathbb{R}\)</span>.</p>
<ol>
<li><p>Let a sample <span class="math inline">\(X_1,\ldots,X_n\)</span>
be given. A random interval <span class="math inline">\(C_n(X) =
[L_n(X_1,\ldots,X_n), U_n(X_1,\ldots,X_n)] \subset \mathbb{R}\)</span>,
where <span class="math inline">\(L_n,U_n\)</span> are measurable
functions, is called a (non-asymptotic) confidence interval (CI) at
level <span class="math inline">\(1 - \alpha \in (0,1)\)</span> for
<span class="math inline">\(\theta\)</span>, if <span
class="math display">\[\forall \theta \in \Theta: \quad
\underbrace{\mathbb{P}_\theta(\theta \in C_n(X))}_{\mathclap{=
\mathbb{P}_\theta\big(L_n(X_1,\ldots,X_n) \leq \theta \leq
U_n(X_1,\ldots,X_n)\big)}} \geq 1 - \alpha.\]</span></p></li>
<li><p>Given a sequence of random intervals <span
class="math inline">\(C_1,C_2,\ldots\)</span> for a sample sequence
<span class="math inline">\(X_1,X_2,\ldots,\)</span> we call <span
class="math inline">\(C_n\)</span> an asymptotic confidence interval if
<span class="math display">\[\forall \theta \in \Theta: \quad \lim_{n
\to \infty} \mathbb{P}_\theta(\theta \in C_n) \geq 1 -
\alpha.\]</span></p></li>
</ol>
</div>
<div class="remark">
<p><em>Remark 1.18</em>. It is important to stress that the confidence
interval <span class="math inline">\(C(X)\)</span> is a random quantity
and the parameter <span class="math inline">\(\theta\)</span> is fixed
(non-random)! The correct interpretation of CIs is that if we construct
many confidence intervals from repeated random samples, at least <span
class="math inline">\(100(1-\alpha)\%\)</span> of these intervals would
contain the true parameter <span
class="math inline">\(\theta\)</span>.</p>
</div>
<h4
id="confidence-interval-for-the-unknown-mean-of-a-population">Confidence
interval for the unknown mean of a population</h4>
<div class="example">
<p><em>Example 1.19</em>. Suppose <span
class="math inline">\(X_1,\dotsc, X_n \overset{i.i.d}{\sim} N(\theta,
1)\)</span>, <span class="math inline">\(\theta \in \Theta \subseteq
\mathbb{R}\)</span>. The interval <span class="math inline">\(C =
(-\infty,\infty)\)</span> is a CI at level <span
class="math inline">\(1-\alpha\)</span> for any <span
class="math inline">\(\alpha \in (0,1)\)</span>, but it is useless in
the sense that it does not help us infer anything about <span
class="math inline">\(\theta\)</span>. A more reasonable way of think
about CIs is viewing it as quantifying the uncertainty of a point
estimator. Consider <span class="math inline">\(\overline{X}_n\)</span>
as an estimator for <span class="math inline">\(\theta\)</span>. Then if
we can show <span class="math display">\[\label{eq:example-1}
      \mathbb{P}_\theta(|\overline{X}_n - \theta| &gt; u_\alpha) &lt;
\alpha,\]</span> then we have <span
class="math inline">\([\overline{X}_n - u_\alpha, \overline{X}_n +
u_\alpha]\)</span> is a CI for <span
class="math inline">\(\theta\)</span> at level <span
class="math inline">\(1-\alpha\)</span>. Indeed, if <a
href="#eq:example-1" data-reference-type="eqref"
data-reference="eq:example-1">[eq:example-1]</a> is true, then we have
<span class="math display">\[\mathbb{P}_\theta(|\overline{X}_n - \theta|
\leq u_\alpha) \geq 1-\alpha \quad
\iff  \mathbb{P}_\theta(\overline{X}_n - u_\alpha \leq \theta \leq
\overline{X}_n + u_\alpha) \geq 1-\alpha.\]</span> In practice, we can
replace <span class="math inline">\(\overline{X}_n\)</span> with the
observed data <span class="math inline">\(\overline{x}_n = \sum_{i=1}^n
x_i/n\)</span>, and the remaining question to find <span
class="math inline">\(u_\alpha\)</span> such that <a
href="#eq:example-1" data-reference-type="eqref"
data-reference="eq:example-1">[eq:example-1]</a> holds. This can be down
using the fact that <span class="math inline">\(\overline{X}_n \sim
N(\theta,1/n)\)</span> and <span
class="math display">\[\mathbb{P}_\theta(|\sqrt{n}|\overline{X}_n -
\theta| &gt; \sqrt{n} u_\alpha) = \mathbb{P}(|Z| &gt;
\sqrt{n}u_{\alpha}) = 2(1-\Phi(\sqrt{n}u_\alpha)),\]</span> where <span
class="math inline">\(Z \sim N(0,1)\)</span> and <span
class="math inline">\(\Phi(z) = \mathbb{P}(Z \leq z)\)</span> is the CDF
of the standard normal distribution. For the above to be less than <span
class="math inline">\(\alpha\)</span>, we need <span
class="math display">\[2(1-\Phi(\sqrt{n}u_\alpha)) \iff u_\alpha &gt;
\frac{1}{\sqrt{n}} \Phi^{-1}(1-\alpha/2).\]</span> For <span
class="math inline">\(\alpha = 0.05\)</span>, <span
class="math inline">\(\Phi^{-1}(1-\alpha/2) = 1.96\)</span>, and
therefore, we conclude that <span class="math display">\[[\overline{X}_n
- \frac{1.96}{\sqrt{n}}, \overline{X}_n +
\frac{1.96}{\sqrt{n}}]\]</span> is a 95% CI for <span
class="math inline">\(\theta\)</span>.</p>
</div>
<div id="example:asym-CI" class="example">
<p><em>Example 1.20</em>. In the example above, the key step in deriving
the CI for <span class="math inline">\(\theta\)</span> is that we use
the distributional property of <span
class="math inline">\(\overline{X}_n\)</span> to control <a
href="#eq:example-1" data-reference-type="eqref"
data-reference="eq:example-1">[eq:example-1]</a>. Now, consider <span
class="math inline">\(X_1,\dotsc,X_n \overset{i.i.d}{\sim}
P_{\theta}\)</span>, <span class="math inline">\(\theta \in \Theta
\subseteq \mathbb{R}\)</span>, with <span
class="math inline">\(\mathbb{E}(X_1) = \theta\)</span> and <span
class="math inline">\(\text{Var}(X_1) = 1\)</span>. Even if we do not
know the distribution <span
class="math inline">\(\overline{X}_n\)</span>, we can still show <span
class="math display">\[[\overline{X}_n - \frac{1.96}{\sqrt{n}},
\overline{X}_n + \frac{1.96}{\sqrt{n}}]\]</span> is an asymptotic 95% CI
for <span class="math inline">\(\theta\)</span> by using CLT and Lemma
<a href="#lem:int_weak" data-reference-type="ref"
data-reference="lem:int_weak">[lem:int_weak]</a>. Indeed, we have <span
class="math inline">\(\sqrt{n}(\overline{X}_n - \theta)
\overset{d}{\longrightarrow} N(0,1)\)</span> by CLT, and therefore <span
class="math display">\[\mathbb{P}_\theta(\sqrt{n}(\overline{X}_n -
\theta) \in [-z,z])  \underset{n \to \infty}{\longrightarrow}  2\Phi(z)
-1,\]</span> and choosing <span class="math inline">\(z =
\Phi^{-1}(1-\alpha/2)\)</span> gives a CI at level <span
class="math inline">\(1-\alpha\)</span>.</p>
</div>
<div id="prop:conf_cheby" class="exercise">
<p><em>Exercise 1.21</em>. Let <span
class="math inline">\(X_1,\dotsc,X_n \overset{i.i.d}{\sim}
P_{\theta}\)</span>, <span class="math inline">\(\theta \in \Theta
\subseteq \mathbb{R}\)</span>, with <span
class="math inline">\(\mathbb{E}(X_1) = \theta\)</span> and <span
class="math inline">\(\text{Var}(X_1) = 1\)</span>.Then, for any <span
class="math inline">\(\alpha \in (0,1)\)</span>, <span
class="math display">\[C_n(X) \coloneq\big[\overline{X}_n -
\tfrac{1}{\sqrt{\alpha n}}, \overline{X}_n + \tfrac{1}{\sqrt{\alpha n}}
\big],\]</span> is an <span class="math inline">\(1-\alpha\)</span>
confidence interval for the unknown mean <span
class="math inline">\(\theta\)</span>.</p>
</div>
<p>Note that the size difference between the asymptotic confidence
intervals <span class="math inline">\(C_n\)</span> from Example <a
href="#example:asym-CI" data-reference-type="ref"
data-reference="example:asym-CI">1.20</a> and the non-asymptotic
confidence intervals from Exercise <a href="#prop:conf_cheby"
data-reference-type="ref" data-reference="prop:conf_cheby">1.21</a> –
denoted for the moment by <span
class="math inline">\(\tilde{C}_n\)</span> – can be large. E.g., for the
typical choice <span class="math inline">\(\alpha = 0.05\)</span>, we
<span class="math inline">\(\lvert \tilde{C}_n \rvert \approx 8.94
n^{-1/2}\)</span>. On the other hand, for the same choice of <span
class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\lvert {C}_n \rvert = 3.92 \sigma
n^{-1/2}\)</span>. Keep in mind however that <span
class="math inline">\(C_n\)</span> only guarantees an asymptotic <span
class="math inline">\(1-\alpha\)</span> coverage and is therefore only
useful for sufficiently large sample sizes, whereas <span
class="math inline">\(\tilde{C}_n\)</span> gives a <span
class="math inline">\(1-\alpha\)</span> coverage in probability for any
<em>finite</em> sample size <span class="math inline">\(n\)</span>.</p>
<p>As we saw above, a common way to establish asymptotic CIs for <span
class="math inline">\(\theta\)</span> is by showing some estimator <span
class="math inline">\(\hat{\theta}_n\)</span> has an asymptotic normal
distribution, as defined below.</p>
<div class="definition">
<p><strong>Definition 1.22</strong>. Let <span
class="math inline">\((P_\theta)_{\theta \in \Theta}\)</span> be a
statistical model with parameter space <span
class="math inline">\(\Theta \subseteq \mathbb{R}\)</span>. We say an
estimator <span class="math inline">\(\hat{\theta}_n\)</span> is
asymptotically normal if, for some <span class="math inline">\(\sigma_n
\rightarrow 0\)</span> as <span class="math inline">\(n \rightarrow
\infty\)</span>, <span class="math display">\[\frac{\hat{\theta}_n-
\theta}{\sigma_n} \overset{d}{\longrightarrow} N(0,1).\]</span></p>
</div>
<div class="example">
<p><em>Example 1.23</em>. Let <span class="math inline">\(X_1,\dotsc,X_n
\overset{i.i.d}{\sim} P_{\theta}\)</span>, <span
class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}\)</span>,
with <span class="math inline">\(\mathbb{E}(X_1) = \theta\)</span> and
<span class="math inline">\(\text{Var}(X_1) = \sigma^2\)</span>. CLT
shows <span class="math inline">\(\hat{\theta}_n=
\overline{X}_n\)</span> is asymptotically normal with <span
class="math inline">\(\sigma_n = \sigma/\sqrt{n}\)</span>. Using the
same arguments as in Example <a href="#example:asym-CI"
data-reference-type="ref" data-reference="example:asym-CI">1.20</a>, we
have <span class="math display">\[[\overline{X}_n -
\frac{1.96\sigma}{\sqrt{n}}, \overline{X}_n +
\frac{1.96\sigma}{\sqrt{n}}]\]</span> is an asymptotic CI for <span
class="math inline">\(\theta\)</span> at level <span
class="math inline">\(0.95\)</span>. In practice, we often do not know
the value of <span class="math inline">\(\sigma\)</span> and it has to
be estimated using the data. However, as long as we have a consistent
estimator for <span class="math inline">\(\sigma\)</span>, i.e. <span
class="math inline">\(\hat{\sigma}_n \overset{P}{\longrightarrow}
    \sigma\)</span> then <span class="math display">\[[\overline{X}_n -
\frac{1.96\hat{\sigma}}{\sqrt{n}}, \overline{X}_n +
\frac{1.96\hat{\sigma}}{\sqrt{n}}]\]</span> is again an asymptotic CI
for <span class="math inline">\(\theta\)</span> at level <span
class="math inline">\(0.95\)</span>. This is because we actually have
<span class="math display">\[\frac{\hat{\theta}_n-
\theta}{\hat{\sigma}_n} \overset{d}{\longrightarrow} N(0,1),\]</span>
due to Slutsky’s theorem.</p>
</div>
<div class="exercise">
<p><em>Exercise 1.24</em>. Given a statistical model <span
class="math inline">\((P_\theta)_{\theta \in \Theta}\)</span> and an
asymptotically normal estimator <span
class="math inline">\(\hat{\theta}_n\)</span>, the interval <span
class="math inline">\([\hat{\theta}_n- \sigma_n \Phi^{-1}(1-\alpha/2),
\hat{\theta}_n+ \sigma_n \Phi^{-1}(1-\alpha/2)]\)</span> is an
asymptotic CI for <span class="math inline">\(\theta\)</span> at level
<span class="math inline">\(1-\alpha\)</span>.</p>
</div>
</body>
</html>
